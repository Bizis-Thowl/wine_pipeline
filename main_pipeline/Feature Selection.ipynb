{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import copy\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_full = pd.read_csv(\"./data/base/train-white.csv\", sep=\",\")\n",
    "test_full = pd.read_csv(\"./data/base/test-white.csv\", sep=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = \"quality\"\n",
    "model_name = \"rf\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./model/prepared-{}.pickle'.format(model_name), 'rb') as f:\n",
    "    model = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = copy.deepcopy(train_full.drop(columns=target))\n",
    "X_test = copy.deepcopy(test_full.drop(columns=target))\n",
    "y_train = train_full[target]\n",
    "y_test = test_full[target]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_run(X, y, X_valid, y_valid, model):\n",
    "    clf = model\n",
    "    clf.fit(X, y)\n",
    "    y_valid_pred = clf.predict(X_valid)\n",
    "    y_train_pred = clf.predict(X)\n",
    "    f1_train = f1_score(y, y_train_pred, average=\"weighted\")\n",
    "    precision_train = precision_score(y, y_train_pred, average=\"weighted\")\n",
    "    recall_train = recall_score(y, y_train_pred, average=\"weighted\")\n",
    "    f1 = f1_score(y_valid, y_valid_pred, average=\"weighted\")\n",
    "    precision = precision_score(y_valid, y_valid_pred, average=\"weighted\")\n",
    "    recall = recall_score(y_valid, y_valid_pred, average=\"weighted\")\n",
    "\n",
    "    scores = {\n",
    "        \"f1_train\": f1_train,\n",
    "        \"precision_train\": precision_train,\n",
    "        \"recall_train\": recall_train,\n",
    "        \"f1\": f1,\n",
    "        \"precision\": precision,\n",
    "        \"recall\": recall,\n",
    "    }\n",
    "\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import f_classif\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn.feature_selection import SelectFromModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_runs():\n",
    "\n",
    "    clf = ExtraTreesClassifier(n_estimators=50)\n",
    "\n",
    "    tree_selector = SelectFromModel(clf)\n",
    "    univariate_selector1 = SelectKBest(f_classif, k=3)\n",
    "    univariate_selector2 = SelectKBest(f_classif, k=6)\n",
    "    univariate_selector3 = SelectKBest(f_classif, k=9)\n",
    "    \n",
    "    base_estimator = model\n",
    "    base_estimator.steps.insert(-1, [\"selector\", tree_selector])\n",
    "    param_grid = {\n",
    "        \"{}__max_depth\".format(model_name): [10, 15, 20, 30],\n",
    "        \"selector\": [\"passthrough\", univariate_selector1, univariate_selector2, univariate_selector3, tree_selector]\n",
    "    }\n",
    "    sh = GridSearchCV(base_estimator, param_grid, scoring=\"f1_weighted\", verbose=2).fit(X_train, y_train)\n",
    "    results = sh.cv_results_\n",
    "    best_clf = sh.best_estimator_\n",
    "    \n",
    "    return best_clf, results, param_grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_clf, results, param_grid = train_runs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = train_run(X_train, y_train, X_test, y_test, best_clf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(best_clf)\n",
    "print(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_train = best_clf.predict(X_train)\n",
    "y_pred_test = best_clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_rates(confusion_matrix):\n",
    "    FP = confusion_matrix.sum(axis=0) - np.diag(confusion_matrix)\n",
    "    FN = confusion_matrix.sum(axis=1) - np.diag(confusion_matrix)\n",
    "    TP = np.diag(confusion_matrix)\n",
    "    TN = confusion_matrix.sum() - (FP + FN + TP)\n",
    "\n",
    "    # Sensitivity, hit rate, recall, or true positive rate\n",
    "    TPR = TP / (TP + FN)\n",
    "    # Specificity or true negative rate\n",
    "    TNR = TN / (TN + FP)\n",
    "    # Precision or positive predictive value\n",
    "    PPV = TP / (TP + FP)\n",
    "    # Negative predictive value\n",
    "    NPV = TN / (TN + FN)\n",
    "    # Fall out or false positive rate\n",
    "    FPR = FP / (FP + TN)\n",
    "    # False negative rate\n",
    "    FNR = FN / (TP + FN)\n",
    "    # False discovery rate\n",
    "    FDR = FP / (TP + FP)\n",
    "\n",
    "    return {\n",
    "        \"tpr\": TPR,\n",
    "        \"tnr\": TNR,\n",
    "        \"ppv\": PPV,\n",
    "        \"npv\": NPV,\n",
    "        \"fpr\": FPR,\n",
    "        \"fnr\": FNR,\n",
    "        \"fdr\": FDR,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create confusion matrix\n",
    "train_conf_mat = confusion_matrix(y_train, y_pred_train)\n",
    "test_conf_mat = confusion_matrix(y_test, y_pred_test)\n",
    "\n",
    "train_rates = compute_rates(train_conf_mat)\n",
    "val_rates = compute_rates(test_conf_mat)\n",
    "\n",
    "classes = np.sort(y_train.unique())\n",
    "classes_val = np.sort(y_test.unique())\n",
    "\n",
    "# create dataframes from confusion matrices\n",
    "train_df = pd.DataFrame(train_conf_mat, index=classes, columns=classes)\n",
    "val_df = pd.DataFrame(test_conf_mat, index=classes_val, columns=classes_val)\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 7))\n",
    "\n",
    "# plot heatmap for training data\n",
    "sns.heatmap(train_df, annot=True, fmt='d', ax=axes[0], cmap='YlGnBu')\n",
    "axes[0].set_title('Train Confusion Matrix')\n",
    "axes[0].set_xlabel('Predicted label')\n",
    "axes[0].set_ylabel('True label')\n",
    "\n",
    "# plot heatmap for test data\n",
    "sns.heatmap(val_df, annot=True, fmt='d', ax=axes[1], cmap='YlGnBu')\n",
    "axes[1].set_title('Test Confusion Matrix')\n",
    "axes[1].set_xlabel('Predicted label')\n",
    "axes[1].set_ylabel('True label')\n",
    "\n",
    "false_positives = \"\"\n",
    "for val_class in classes_val:\n",
    "    index = val_class - 3\n",
    "    false_positives += \"{}: {}, \".format(val_class, train_rates[\"fpr\"][index])\n",
    "print(\"false positives: {}\".format(false_positives))\n",
    "true_negatives = \"\"\n",
    "for val_class in classes_val:\n",
    "    index = val_class - 3\n",
    "    true_negatives += \"{}: {}, \".format(val_class, train_rates[\"tnr\"][index])\n",
    "print(\"true negatives: {}\".format(true_negatives))\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import learning_curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_size_abs, train_scores, test_scores = learning_curve(\n",
    "    best_clf, X_train, y_train, scoring=\"f1_weighted\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for train_size, cv_train_scores, cv_test_scores in zip(\n",
    "    train_size_abs, train_scores, test_scores\n",
    "):\n",
    "    print(f\"{train_size} samples were used to train the model\")\n",
    "    print(f\"The average train f1_weighted is {cv_train_scores.mean():.2f}\")\n",
    "    print(f\"The average test f1_weighted is {cv_test_scores.mean():.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_lc = []\n",
    "for train_size, cv_train_scores, cv_test_scores in zip(\n",
    "    train_size_abs, train_scores, test_scores\n",
    "):\n",
    "    my_lc.append(\n",
    "        {\n",
    "            \"n\": int(train_size),\n",
    "            \"train_score\": cv_train_scores.mean(),\n",
    "            \"test_score\": cv_test_scores.mean(),\n",
    "        }\n",
    "    )\n",
    "\n",
    "my_lc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys; sys.path.insert(0, '..')\n",
    "from utils.mongo.MongoModelHandler import MongoModelHandler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mongo_model_handler = MongoModelHandler(\"selection\", model_name=model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mongo_model_handler.store_model(best_clf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mongo_model_handler.store_model_scores(general=scores, conf_matrix_train=train_df, conf_matrix_test=val_df, learning_curve=my_lc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mongo_model_handler.store_train_process(strategies=param_grid, approach=\"sklearn.model_selection.GridSearchCV\", metric=\"f1_weighted\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mongo_model_handler.store_train_configs(results=results)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom Reset of Mongo Collections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from bson.objectid import ObjectId"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# train_config_collection = mongo_model_handler.train_config_collection\n",
    "\n",
    "# train_config_collection.delete_many({\"parameters\": {\"rf__max_depth\": \"5\"}})\n",
    "# train_config_collection.delete_many({\"parameters\": {\"rf__max_depth\": \"10\"}})\n",
    "# train_config_collection.delete_many({\"parameters\": {\"rf__max_depth\": \"20\"}})"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('./model/selected-{}.pickle'.format(model_name), 'wb') as handle:\n",
    "    handle.write(pickle.dumps(best_clf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "vscode": {
   "interpreter": {
    "hash": "d6c020baedb207c2cc99e907008bdca3bc7007f1edfa10331d8acee936d701c4"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
